# -*- coding: utf-8 -*-
"""RAG_based-Application.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E8G9A5_4hDU6nNYrAb6-eAgT7Z_edcot
"""

!pip install -U langchain-community

!pip install sentence-transformers

!pip install pymupdf

!pip install -q langchain openai chromadb unstructured faiss-cpu tiktoken

from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Load PDF file
loader = PyMuPDFLoader("sample_data/quant_dev_trial.pdf")  # Replace with your filename
documents = loader.load()

# Split into manageable chunks
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    separators=["\n\n", "\n", " ", ""]
)
chunks = text_splitter.split_documents(documents)

print(f"Total Chunks: {len(chunks)}")
print(chunks[0])

from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings

# Use free local model (MiniLM is fast & decent)
embedding = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

# Generate and store embeddings
vectorstore = FAISS.from_documents(chunks, embedding)

# Save the index locally (optional)
vectorstore.save_local("faiss_index")

!pip install llama-cpp-python

!wget https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf -O mistral.gguf

from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings

# Use a free & fast local model
embedding = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

# Create the vector store and generate embeddings
vectorstore = FAISS.from_documents(chunks, embedding)

# Save the index locally (optional)
vectorstore.save_local("faiss_index")

from llama_cpp import Llama

llm = Llama(
    model_path="./mistral.gguf",
    n_ctx=2048,
    n_threads=8  # Colab CPU
)

prompt = "Explain what a RAG pipeline is in NLP."
response = llm(prompt, max_tokens=300, stop=["\n"])

print(response["choices"][0]["text"])



!pip install sentence-transformers

def make_rag_prompt(context, question):
    return f"""Use the following context to answer the question.

Context:
{context}

Question:
{question}

Answer:"""

context = """
Retrieval-Augmented Generation (RAG) is an NLP technique that enhances the performance of language models by incorporating external knowledge retrieved from a database or document store.
"""

question = "What is the main benefit of using RAG in language models?"
prompt = make_rag_prompt(context, question)

response = llm(prompt, max_tokens=200)
print(response["choices"][0]["text"])

!pip install pymupdf faiss-cpu sentence-transformers langchain

from google.colab import files
uploaded = files.upload()

import fitz  # PyMuPDF

def extract_text_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    text = ""
    for page in doc:
        text += page.get_text()
    return text

pdf_text = extract_text_from_pdf("quant_dev_trial.pdf")
print(pdf_text[:1000])  # Preview

from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)

chunks = splitter.split_text(pdf_text)
print(f"Number of chunks: {len(chunks)}")
print(chunks[0])

from sentence_transformers import SentenceTransformer

embed_model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = embed_model.encode(chunks, show_progress_bar=True)

import faiss
import numpy as np

dimension = embeddings[0].shape[0]
index = faiss.IndexFlatL2(dimension)
index.add(np.array(embeddings))

print(f"Added {index.ntotal} vectors to FAISS index.")

def retrieve_top_chunks(query, k=3):
    query_vec = embed_model.encode([query])
    scores, indices = index.search(np.array(query_vec), k)
    return [chunks[i] for i in indices[0]]

query = "What is meaning of MBO in this document?"
top_chunks = retrieve_top_chunks(query)
context = "\n".join(top_chunks)

rag_prompt = f"""Use the context below to answer the question.

Context:
{context}

Question: {query}

Answer:"""

response = llm(rag_prompt, max_tokens=300)
print(response["choices"][0]["text"])

